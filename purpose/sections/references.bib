@Article{Bhomik2018Clustering,
  author = {Bhowmik, Debsindhu and \textbf{M. Todd Young} and Gao, Shang and Ramanathan, Arvind},
  title = {Deep Clustering of Protein Folding Simulations},
  journal = {BMC Bioinformatics},
  year = {2018},
  month = {06},
  pages = {},
  doi = {10.1101/339879}
}

@inproceedings{bergstra2011algorithms,
  title={Algorithms for hyper-parameter optimization},
  author={Bergstra, James S and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal{\'a}zs},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2546--2554},
  year={2011}
}

@inproceedings{ChapelleL11:News,
  author    = {Olivier Chapelle and
               Lihong Li},
  title     = {An Empirical Evaluation of Thompson Sampling},
  booktitle = {{NIPS}},
  pages     = {2249--2257},
  year      = {2011}
}

@inproceedings{Henderson:RLMatters,
  author    = {Peter Henderson and
               Riashat Islam and
               Philip Bachman and
               Joelle Pineau and
               Doina Precup and
               David Meger},
  title     = {Deep Reinforcement Learning That Matters},
  booktitle = {{AAAI}},
  pages     = {3207--3214},
  publisher = {{AAAI} Press},
  year      = {2018}
}

@Article{Gao2017HAN,
  author = {Gao, Shang and \textbf{M. Todd Young} and X Qiu, John and Yoon, Hong-Jun and B Christian, James and A Fearn, Paul and Tourassi, Georgia and Ramanathan, Arvind},
  title = {Hierarchical attention networks for information extraction from cancer pathology reports},
  journal = {Journal of the American Medical Informatics Association : JAMIA},
  year = {2017},
  month = {11},
  pages = {},
  volume = {25},
  doi = {10.1093/jamia/ocx131}
}

@inproceedings{gelbart2014,
author= {Gelbart, Michael A. and Snoek, Jasper and Adams, Ryan P.},
 title = {Bayesian Optimization with Unknown Constraints},
 booktitle = {Proceedings of the Thirtieth Conference on Uncertainty in Artificial Intelligence},
 series = {UAI'14},
 year = {2914},
 isbn = {978-0-9749039-1-0},
 location = {Quebec City, Quebec, Canada},
 pages = {250--259},
 numpages = {10},
 acmid = {3020778},
 publisher = {AUAI Press},
 address = {Arlington, Virginia, United States},
} 

@inproceedings{Lam:Aero,
author = {Lam, Remi and Poloczek, Matthias and Frazier, Peter and E. Willcox, Karen},
year = {2018},
month = {01},
pages = {},
title = {Advances in Bayesian Optimization with Applications in Aerospace Engineering},
doi = {10.2514/6.2018-1656}
}

@inproceedings{Li:2010:NewsBandits,
 author = {Li, Lihong and Chu, Wei and Langford, John and Schapire, Robert E.},
 title = {A Contextual-bandit Approach to Personalized News Article Recommendation},
 booktitle = {Proceedings of the 19th International Conference on World Wide Web},
 series = {WWW '10},
 year = {2010},
 isbn = {978-1-60558-799-8},
 location = {Raleigh, North Carolina, USA},
 pages = {661--670},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1772690.1772758},
 doi = {10.1145/1772690.1772758},
 acmid = {1772758},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {contextual bandit, exploration/exploitation dilemma, personalization, recommender systems, web service},
} 

@article{MachadoBTVHB18,
  author    = {Marlos C. Machado and
               Marc G. Bellemare and
               Erik Talvitie and
               Joel Veness and
               Matthew J. Hausknecht and
               Michael Bowling},
  title     = {Revisiting the Arcade Learning Environment: Evaluation Protocols and
               Open Problems for General Agents},
  journal   = {J. Artif. Intell. Res.},
  volume    = {61},
  pages     = {523--562},
  year      = {2018}
}

@article{Mnih:Atari,
  author    = {Ionel{-}Alexandru Hosu and
               Traian Rebedea},
  title     = {Playing Atari Games with Deep Reinforcement Learning and Human Checkpoint
               Replay},
  journal   = {CoRR},
  volume    = {abs/1607.05077},
  year      = {2016}
}

@inproceedings{Mockus:1974,
  author = {Mockus, Jonas},
  title = {On Bayesian Methods for Seeking the Extremum},
  booktitle = {Proceedings of the IFIP Technical Conference},
  year = {1974},
  isbn = {3-540-07165-2},
  pages = {400--404},
} 

@article{Recht:Tour,
  author    = {Benjamin Recht},
  title     = {A Tour of Reinforcement Learning: The View from Continuous Control},
  journal   = {CoRR},
  volume    = {abs/1806.09460},
  year      = {2018}
}

@article {Silver1140,
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
	title = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
	volume = {362},
	number = {6419},
	pages = {1140--1144},
	year = {2018},
	doi = {10.1126/science.aar6404},
	publisher = {American Association for the Advancement of Science},
	abstract = {Computers can beat humans at increasingly complex games, including chess and Go. However, these programs are typically constructed for a particular game, exploiting its properties, such as the symmetries of the board on which it is played. Silver et al. developed a program called AlphaZero, which taught itself to play Go, chess, and shogi (a Japanese version of chess) (see the Editorial, and the Perspective by Campbell). AlphaZero managed to beat state-of-the-art programs specializing in these three games. The ability of AlphaZero to adapt to various game rules is a notable step toward achieving a general game-playing system.Science, this issue p. 1140; see also pp. 1087 and 1118The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.},
	issn = {0036-8075},
	URL = {http://science.sciencemag.org/content/362/6419/1140},
	eprint = {http://science.sciencemag.org/content/362/6419/1140.full.pdf},
	journal = {Science}
}

@book{Sutton:1998,
 author = {Sutton, Richard S. and Barto, Andrew G.},
 title = {Introduction to Reinforcement Learning},
 year = {1998},
 isbn = {0262193981},
 edition = {1st},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@inproceedings{VanchinathanNBK14:recommender,
  author    = {Hastagiri P. Vanchinathan and
               Isidor Nikolic and
               Fabio De Bona and
               Andreas Krause},
  title     = {Explore-exploit in top-N recommender systems via Gaussian processes},
  booktitle = {RecSys},
  pages     = {225--232},
  publisher = {{ACM}},
  year      = {2014}
}

@InProceedings{Young2018Hyperspace,
  author =    {M. Todd Young and Hinkle, Jacob and Kannan, Ramakrishnan and Ramanathan, Arvind},
  title =     {HyperSpace: Distributed Bayesian Hyperparameter Optimization},
  booktitle = {High Performance Machine Learning Workshop},
  year =      {2018},
  pages =     {},
  address =   {},
  month =     {},
  publisher = {IEEE Xplore},
  doi =       {},
  isbn =      {},
  url =       {}
}