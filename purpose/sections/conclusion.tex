\section*{Future Directions}
My prior work has focused on using Bayesian optimization as a tool to steer research in machine learning. This is valuable for fundamental research and has led to progress in molecular dynamics simulations \cite{Bhomik2018Clustering} and natural language processing \cite{Gao2017HAN}. However, as machine learning algorithms become more ubiquitous, I believe there is an increasing need to incorporate human feedback into these systems. If we are to create more robust and safe machine learning systems, I believe that their designs must account for the fact that the lives and needs of the people interacting with them are dynamic and a potential source of knowledge. There has been some promising initial work that has shown it possible to interject human preferences in RL algorithms where it is difficult to specify a reward function \cite{ChristianoLBMLA17}. I believe that this line of research has the potential to improve learning algorithms and has the potential to make them safer. Imagine if a passenger of an autonomous vehicle could inform the learning algorithm when she believes the algorithm to be behaving dangerously. This correction would not only improve the learning algorithm, but also return a sense of autonomy to the passenger. This return of control to the people interacting with learning algorithms would promote a sense collaboration with these learning systems. In a world that is increasingly being moved by machine learning, I believe it important for all individuals, experts and non-experts alike, to have a hand in steering the direction of our increasingly autonomous future.